Connecting Once you submit your proposal and are approved access you will be able to connect to our Slurm master node via SSH by running the following command: ssh my_ocf_username@hpcctl.ocf.berkeley.edu If you have trouble connecting please contact us at help@ocf.berkeley.edu , or come to staff hours when the lab is open and chat with us in person.

We also have a #hpc_users channel on slack and irc where you can ask questions and talk to us about anything HPC.

The Cluster As of Fall 2023, the OCF HPC cluster is composed of one server, with the following specifications: 2 Intel Xeon Platinum 8352Y CPUs (32c/64t @ 2.4GHz) 4 NVIDIA RTX A6000 GPUs 256GB ECC DDR4-3200 RAM The current hardware were funded with our ASUC budget and the GPUs were gifted by NVIDIA through the [NVIDIA Academic Hardware Grant Program] ( https://developer.nvidia.com/higher-education-and-research ). Slurm We currently use Slurm as our workload manager for the cluster.